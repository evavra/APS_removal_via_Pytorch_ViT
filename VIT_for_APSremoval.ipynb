{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670e0016-df0d-41d3-8366-ac544adc4557",
   "metadata": {},
   "source": [
    "Title: VIT_for_APSremoval   \n",
    "Author: Luke Fairbanks     \n",
    "\n",
    "Credit: Yuri Fialko for project guidance & data; https://sioviz.ucsd.edu/~fialko/     \n",
    "- the original training data for this ML model was from some matlab code of his meant to replicate APS (atmospherica phase screen) noise in InSAR images     \n",
    "- the testing/development dataset is relatively simple, 40x40 images with two classes (datm - signal and noise, gatm - noise)   \n",
    "- where the signal is a simple gaussian bump & the noise is randomized spatial noise across the pixels of the image, obscurring the underlying signal   \n",
    "- one of key points is that this noise screen varies with time whereas the underlying signal is expected to be a more persistent signal wrt time  \n",
    "- hence the structure of the ML model to take advantage of 'multiplicity' parameter to simulate 'moving window' of data    \n",
    "\n",
    "Credit: Pytorch libraries & others which form basis for model; https://pytorch.org/     \n",
    "\n",
    "Credit: https://github.com/lucidrains/vit-pytorch authors & the parent libraries they pull from;      \n",
    "- they organized a host of ViT architectures as examples, these example files should be in the 'vit_pytorch' folder, more info at bottom     \n",
    "- my ViT is different in some fundamental ways, namely it does not do classification but rather filtration/transform    \n",
    "- the whole point is to take InSAR images, or in this case test images, and identify the persistent signal (fault line movement of Earth's crust)   \n",
    "- where the noise over the image \n",
    "- effectively what my model does is take as input an image which has signal and noise, passes the image through a visual transfromer, the compares the output, the transformed image, to the residual signal which is SignalAndNoise - Noise data    \n",
    "- comparison is done via MSE across the pixel values, where model(SignalAndNoise) and Residues have same dimensionality   \n",
    "- then this MSE loss is used as the model training loss   \n",
    "- code is structured to create proper dimensions but for clarity tensors have dimensionality (batchNum, channelNum, IMGheight, IMGwidth)   \n",
    "- I'm going to try & automate as much as I can, and will show where user input is expected to best of my ability   \n",
    "- goal is to minimize it down to data folder root address input and parameter inputs   \n",
    "- 'multiplicity' is an important parameter to use methodology of CSS (APS removal) since it simulates taking 1 resuidue/signal image(long lasting in time), and re-sampling spatial noise across the corresponding SignalAndNoise image   \n",
    "- the whole idea being one may increase 'multiplicity' (integer), increasing channelNum of tensor, allowing model to train/test on arbitrary length (temporal-simulated/noise-resampled) sequences of images   \n",
    "- this procedure effectively allows ViT model to look at 'moving window' of data as if from previous N=multiplicity image snapshots to extract the long lasting features of the image   \n",
    "- pixel variance of original SignalAndNoise image is used to re-sample noise w/ pytorch methods,    \n",
    "- check last section for more details on 'what next?'    \n",
    "- after you've got the code running on example data provided, please continue running cells   \n",
    "- contact Luke Fairbanks for questions; otherwise try code debuggers or chatbot-assisted debug/testing, pytorch/torchvision documentation, example code from git/lucidrains & other similar projects, etc...     \n",
    "\n",
    "\n",
    "If you wish to run the code on simply the example data and get a nice 'clean run' of the code, please load imports below, run the function/class definition block, then proceed to [section 4] to train the model via the function 'trainModelFromScratch'    \n",
    "\n",
    "Cheers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e12d29-edbc-4985-8e0a-232fd6690e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "#please run & continue or download libraries used via PIP or other methods\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from skimage import io, transform\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import transforms, utils\n",
    "# from torchvision.datasets import ImageFolder\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from einops import rearrange\n",
    "# from einops.layers.torch import Rearrange\n",
    "# import xarray as xr\n",
    "# from PIL import Image\n",
    "# import shutil\n",
    "# import imageio\n",
    "\n",
    "from VIT_for_APSremoval import trainModelFromScratch\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b470d-e526-49db-a063-83f9b4bb5848",
   "metadata": {},
   "source": [
    "The following code block encapsulates all functions\n",
    "\n",
    "##############################################################################################################################################################\n",
    "\n",
    "\n",
    "[section 1]  \n",
    "\n",
    "The first code (top of cell above and immediately below) regard pre-processing data    \n",
    "This code assumes output from the fake_igrams.m file, namely sets of images in subdirectories datm(noise+signal)/gatm(noise) OR 'raw' images paired with CSS result images     \n",
    "You should be able to simply SPECIFY      \n",
    "- your data address folder within 'USER_fake_igrams_address' or change the example code address DEFAULT_DATM_GATM_ROOTaddress to the root where data is stored  \n",
    "    - easier to just modify DEFAULT_DATM_GATM_ROOTaddress\n",
    "- image dimensions with (x,y,channelNum with imgDim)   \n",
    "- modify\n",
    "\n",
    "If using 'real' data, would assume only having Signal+Noise, and possibly a CSS stack result as the best standardized approximation of underlying signal    \n",
    "in this case,     \n",
    "- use raw images as SignalAndNoise images (datm) and subtact CSS result from SignalAndNoise to make substitue for noise (gatm), use [load data function], [matrix arithmetic function], and [save data into datm/gatm format function] to perform this operation    \n",
    "- OR use the CSS result as the Residue = SignalAndNoise - Noise, and proceed to the machine learning section (2) of the code by running [function to organize SignalAndNoise and Residue for training].     \n",
    "\n",
    "Make sure to run the definitions of classes and functions immediately below & proceed to model training after prepping data    \n",
    "\n",
    "The code immediately below is included for clarity about data prep process; however, for example code and use one may proceed to 'trainModelFromScratch' call\n",
    "\n",
    "##############################################################################################################################################################\n",
    "\n",
    "\n",
    "[section 2]\n",
    "\n",
    "The second block of code in definitions^ regards the machine learning model\n",
    "You should be able to simply run the code & proceed to training\n",
    "\n",
    "##############################################################################################################################################################\n",
    "\n",
    "\n",
    "[section 3]\n",
    "\n",
    "functions to help user plot things and save results\n",
    "\n",
    "##############################################################################################################################################################\n",
    "\n",
    "\n",
    "please run the code block & once successful proceed to  [section 4] to run the data-prep, training, and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51eda5a5-0726-49a2-8757-bafb2057adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################################################################################################################################################\n",
    "# #helper functions\n",
    "\n",
    "\n",
    "# #loader to get datm/gatm address & return allNoise, allSignalNoise, allResidues np arrays\n",
    "# def DATM_GATM_loader_NParrays(DATM_GATM_address, imgDimensions):    #example input DEFAULT_DATM_GATM_address, imgDimensions = [x,y,channel=1(greyscale)]\n",
    "#     folderNoise = DATM_GATM_address + 'gatm/'  ##noise only = gatm\n",
    "#     folderSignalandNoise = DATM_GATM_address + 'datm/' ##signal+noise = datm\n",
    "#     filesSN = os.listdir(folderSignalandNoise) #list of the files\n",
    "#     filesN = os.listdir(folderNoise)\n",
    "#     filesSN = [file for file in filesSN if os.path.isfile(os.path.join(folderSignalandNoise, file))]\n",
    "#     filesN = [file for file in filesN if os.path.isfile(os.path.join(folderNoise, file))]\n",
    "#     img_rows, img_cols, channels = imgDimensions  #define parameters for image size\n",
    "#     allNoise = np.zeros((img_rows, img_cols,len(filesN)))   # create numpy arrays to store all images from the folders\n",
    "#     allSignalNoise = np.zeros((img_rows, img_cols,len(filesSN))) \n",
    "#     #loops to fill the allNoise and allSignalNoise arrays with the image data, this is done for testing, but also since it's easy to then convert these to png files for the pytorch/torchvision training\n",
    "#     count=0\n",
    "#     for file in filesN:\n",
    "#         pathNow = folderNoise + file\n",
    "#         filNow = xr.open_dataset(pathNow, engine='netcdf4')\n",
    "#         Znow = filNow['z']\n",
    "#         allNoise[:,:,count] = Znow\n",
    "#         count += 1    \n",
    "#     count=0\n",
    "#     for file in filesSN:\n",
    "#         pathNow = folderSignalandNoise + file\n",
    "#         filNow = xr.open_dataset(pathNow, engine='netcdf4')\n",
    "#         Znow = filNow['z']\n",
    "#         allSignalNoise[:,:,count] = Znow\n",
    "#         count += 1 \n",
    "#     #create an array to store the residual data => the signal    \n",
    "#     allResidues = allSignalNoise - allNoise   \n",
    "#     return allNoise, allSignalNoise, allResidues\n",
    "\n",
    "\n",
    "# def RAW_CSS_loader_NParrays(root_address, imgDimensions):\n",
    "#     folderSignalandNoise = DATM_GATM_address + 'raw/' ##signal+noise = RAW\n",
    "#     folderResidues = DATM_GATM_address + 'css/' ##residue = CSS stack result = approximation of interferomatric signal image\n",
    "#     filesSN = os.listdir(folderSignalandNoise) #list of the files\n",
    "#     filesR = os.listdir(folderResidues)\n",
    "#     filesSN = [file for file in filesSN if os.path.isfile(os.path.join(folderSignalandNoise, file))]\n",
    "#     filesN = [file for file in filesN if os.path.isfile(os.path.join(folderNoise, file))]\n",
    "#     filesR = [file for file in filesR if os.path.isfile(os.path.join(folderResidues, file))]\n",
    "#     img_rows, img_cols, channels = imgDimensions  #define parameters for image size\n",
    "#     allSignalNoise = np.zeros((img_rows, img_cols,len(filesSN))) \n",
    "#     allResidues = np.zeros((img_rows, img_cols,len(filesSN)))\n",
    "#     #loops to fill the allNoise and allSignalNoise arrays with the image data, this is done for testing, but also since it's easy to then convert these to png files for the pytorch/torchvision training   \n",
    "#     count=0\n",
    "#     for file in filesSN:\n",
    "#         pathNow = folderSignalandNoise + file\n",
    "#         filNow = xr.open_dataset(pathNow, engine='netcdf4')\n",
    "#         Znow = filNow['z']\n",
    "#         allSignalNoise[:,:,count] = Znow\n",
    "#         count += 1 \n",
    "#     count=0\n",
    "#     for file in filesR:\n",
    "#         pathNow = folderResidues + file\n",
    "#         filNow = xr.open_dataset(pathNow, engine='netcdf4')\n",
    "#         Znow = filNow['z']\n",
    "#         allResidues[:,:,count] = Znow\n",
    "#         count += 1\n",
    "#     #create an array to store the residual data => the signal      \n",
    "#     allNoise = allSignalNoise - allResidues\n",
    "#     return allNoise, allSignalNoise, allResidues\n",
    "\n",
    "\n",
    "# # Function to normalize an array and convert it to uint8\n",
    "# def normalize_and_convert(array):\n",
    "#     array = (array - np.min(array)) / (np.max(array) - np.min(array)) * 255\n",
    "#     return array.astype('uint8')\n",
    "\n",
    "\n",
    "# #loader to get datm/gatm address & prepare the directory for use in pytorch\n",
    "# def directoryPrep(DATM_GATM_address, imgDimensions, setting = 'DATM_GATM'):\n",
    "#     if setting == 'RAW_CSS':\n",
    "#         allNoise, allSignalNoise, allResidues = RAW_CSS_loader_NParrays(DATM_GATM_address, imgDimensions)\n",
    "#     elif setting == 'DATM_GATM':\n",
    "#         allNoise, allSignalNoise, allResidues = DATM_GATM_loader_NParrays(DATM_GATM_address, imgDimensions)\n",
    "#     #count the number of images of each type; noise, signal+noise, and residue\n",
    "#     #should be equal\n",
    "#     nCount = allNoise.shape[2]\n",
    "#     snCount = allSignalNoise.shape[2]\n",
    "#     rCount = allResidues.shape[2] \n",
    "#     #organize process of saving images as png in the home directory, in a structure which works well with pytorch data loader\n",
    "#     folderBase = DATM_GATM_address  #'D:\\\\Research\\\\Yuri Fialko\\\\set 2\\\\'\n",
    "#     nSub = 'NoisePng/'\n",
    "#     snSub = 'SignalNoisePng/'\n",
    "#     rSub = 'ResiduePng/'\n",
    "#     allFolder = folderBase + 'Png_n_sn_r/'\n",
    "#     nSubFull = folderBase + nSub\n",
    "#     snSubFull = folderBase + snSub\n",
    "#     rSubFull = folderBase + rSub\n",
    "#     #create directories if they do not exist yet\n",
    "#     if not os.path.exists(nSubFull):\n",
    "#         os.makedirs(nSubFull)\n",
    "#     if not os.path.exists(snSubFull):\n",
    "#         os.makedirs(snSubFull)\n",
    "#     if not os.path.exists(rSubFull):\n",
    "#         os.makedirs(rSubFull)\n",
    "#     if not os.path.exists(allFolder):\n",
    "#         os.makedirs(allFolder)\n",
    "#     # Save the images as PNG files\n",
    "#     for i in range(nCount):\n",
    "#         imgNow = allNoise[:,:,i]\n",
    "#         imgNow = normalize_and_convert(imgNow)\n",
    "#         imgNow2 = Image.fromarray(imgNow.astype('uint8'))\n",
    "#         imgNow2.save(nSubFull + 'n' + str(i) + '.png')\n",
    "#         imgNow2.save(allFolder + 'n' + str(i) + '.png')\n",
    "#     for i in range(snCount):\n",
    "#         imgNow = allSignalNoise[:,:,i]\n",
    "#         imgNow = normalize_and_convert(imgNow)\n",
    "#         imgNow2 = Image.fromarray(imgNow.astype('uint8'))\n",
    "#         imgNow2.save(snSubFull + 'sn' + str(i) + '.png')\n",
    "#         imgNow2.save(allFolder + 'sn' + str(i) + '.png')\n",
    "#     for i in range(rCount):\n",
    "#         imgNow = allResidues[:,:,i]\n",
    "#         imgNow = normalize_and_convert(imgNow)\n",
    "#         imgNow2 = Image.fromarray(imgNow.astype('uint8'))\n",
    "#         imgNow2.save(rSubFull + 'r' + str(i) + '.png')\n",
    "#         imgNow2.save(allFolder + 'r' + str(i) + '.png')\n",
    "#     print('Directories should be prepped with PNG files now')\n",
    "#     return None\n",
    "    \n",
    "    \n",
    "# #function to load in data from a folder with all the png files together, namely the folder saved previously \"allFolder = folderBase + 'Png_n_sn_r\\\\'\"    \n",
    "# def organize_images_for_torchvision_APS(directory, train_test_split=0.8):\n",
    "#     # Create directories for the classes in train and test\n",
    "#     PNGallFolder =  directory+'Png_n_sn_r/'\n",
    "#     os.makedirs(os.path.join(PNGallFolder, 'train', 'class_0'), exist_ok=True)   #class_0 = residues\n",
    "#     os.makedirs(os.path.join(PNGallFolder, 'train', 'class_1'), exist_ok=True)   #class_1 = signal+noise\n",
    "#     os.makedirs(os.path.join(PNGallFolder, 'train', 'class_2'), exist_ok=True)   #class_2 = noise\n",
    "#     os.makedirs(os.path.join(PNGallFolder, 'test', 'class_0'), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(PNGallFolder, 'test', 'class_1'), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(PNGallFolder, 'test', 'class_2'), exist_ok=True)\n",
    "#     # Create a list of filenames for each class\n",
    "#     filenames_class_0 = [filename for filename in os.listdir(PNGallFolder) if filename.endswith('.png') and 'r' in filename]\n",
    "#     filenames_class_1 = [filename for filename in os.listdir(PNGallFolder) if filename.endswith('.png') and 'sn' in filename]\n",
    "#     filenames_class_2 = [filename for filename in os.listdir(PNGallFolder) if filename.endswith('.png') and 'r' not in filename and 'sn' not in filename]\n",
    "#     # Sort the filenames\n",
    "#     filenames_class_0.sort()\n",
    "#     filenames_class_1.sort()\n",
    "#     filenames_class_2.sort()\n",
    "#     # Calculate the train/test split index for each class\n",
    "#     split_index_class_0 = int(len(filenames_class_0) * train_test_split)\n",
    "#     split_index_class_1 = int(len(filenames_class_1) * train_test_split)\n",
    "#     split_index_class_2 = int(len(filenames_class_2) * train_test_split)\n",
    "#     # Split the filenames into train and test for each class\n",
    "#     train_filenames_class_0 = filenames_class_0[:split_index_class_0]\n",
    "#     test_filenames_class_0 = filenames_class_0[split_index_class_0:]\n",
    "#     train_filenames_class_1 = filenames_class_1[:split_index_class_1]\n",
    "#     test_filenames_class_1 = filenames_class_1[split_index_class_1:]\n",
    "#     train_filenames_class_2 = filenames_class_2[:split_index_class_2]\n",
    "#     test_filenames_class_2 = filenames_class_2[split_index_class_2:]\n",
    "#     # Move files to the appropriate class directories in the train folder\n",
    "#     for filename in train_filenames_class_0:\n",
    "#         shutil.move(os.path.join(PNGallFolder, filename), os.path.join(PNGallFolder, 'train', 'class_0', filename))\n",
    "#     for filename in train_filenames_class_1:\n",
    "#         shutil.move(os.path.join(PNGallFolder, filename), os.path.join(PNGallFolder, 'train', 'class_1', filename))\n",
    "#     for filename in train_filenames_class_2:\n",
    "#         shutil.move(os.path.join(PNGallFolder, filename), os.path.join(PNGallFolder, 'train', 'class_2', filename))\n",
    "#     # Move files to the appropriate class directories in the test folder\n",
    "#     for filename in test_filenames_class_0:\n",
    "#         shutil.move(os.path.join(PNGallFolder, filename), os.path.join(PNGallFolder, 'test', 'class_0', filename))\n",
    "#     for filename in test_filenames_class_1:\n",
    "#         shutil.move(os.path.join(PNGallFolder, filename), os.path.join(PNGallFolder, 'test', 'class_1', filename))\n",
    "#     for filename in test_filenames_class_2:\n",
    "#         shutil.move(os.path.join(PNGallFolder, filename), os.path.join(PNGallFolder, 'test', 'class_2', filename))\n",
    "#     return None    \n",
    "\n",
    "\n",
    "# def loaderFULL(root_address, imgDimensions, train_test_ratio=0.8, setting='DATM_GATM'):\n",
    "#     if setting == 'DATM_GATM':\n",
    "#         directoryPrep(root_address, imgDimensions, setting)\n",
    "#         organize_images_for_torchvision_APS(root_address, train_test_split=train_test_ratio)\n",
    "#     elif setting == 'RAW_CSS':\n",
    "#         directoryPrep(root_address, imgDimensions, setting)\n",
    "#         organize_images_for_torchvision_APS(root_address, train_test_split=train_test_ratio)\n",
    "#     print('Data should be prepped into train/test subdirectories with class directories within each for noise, signal, and residue classes of images')\n",
    "#     return None\n",
    "    \n",
    "\n",
    "# ###########################################################################################################################################################################\n",
    "\n",
    "\n",
    "# ###########################################################################################################################################################################\n",
    "# #The following code block encapsulates the machine learning class architecture based on the simpleViT example from https://github.com/lucidrains/vit-pytorch\n",
    "# #my model changes are within the 'SimpleViTaps' class where I have changed the model from classification to filtration/transform\n",
    "# #i left all the original architecture intact since why not & also might help if one wants to utilize the other ViT model examples & modify any of them please refer to differences between SimpleViT and SimpleViTaps\n",
    "\n",
    "# \"\"\"\n",
    "# Note, the following SimpleViT architecture has been sourced from https://github.com/lucidrains/vit-pytorch\n",
    "# My modifications are encapsulated within 'SimpleViTaps' where I change the classification model to transform/filtration via model head change\n",
    "# such that rather than returning classification vectors model returns image tensor for     loss( model( SignalAndNoise) , Residual = SignalAndNoise - Noise) [MSE loss]\n",
    "\n",
    "\n",
    "# this is done within:\n",
    "\n",
    "# self.linear_head = nn.Linear(self.dim, self.channels * self.image_height * self.image_width)\n",
    "\n",
    "# x = self.linear_head(x)\n",
    "# x = x.view(-1, self.channels, self.image_height, self.image_width)\n",
    "# return x  \n",
    "\n",
    "\n",
    "# The CustomDatasetAPS is my creation, but heavily inspired by examples in pytorch references\n",
    "# It serves as the system to pick files from the train/test => class_0/class_1/class_2/... folders and load them via the pytorch dataloader for batch training\n",
    "# One of the important functions of my code is the use of 'multiplicity' parameter within this dataset class\n",
    "# Please refer to class definition or documentation at bottom of page for details and advice regarding use of multiplicity parameter and CustomDatasetAPS\n",
    "# Basic idea is that it takes single image sets with SignalAndNoise and Residual, resamples spatial noise abd creates new channels containing these resamples, extending tensor\n",
    "# Meant to simulate method of CSS since can stack up images with resampled noise to have ML model pick up on the long lasting feature of signal\n",
    "# \"\"\"\n",
    "\n",
    "# # helpers (functions)\n",
    "\n",
    "# def pair(t):\n",
    "#     return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
    "#     y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
    "#     assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "#     omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
    "#     omega = 1.0 / (temperature ** omega)\n",
    "#     y = y.flatten()[:, None] * omega[None, :]\n",
    "#     x = x.flatten()[:, None] * omega[None, :]\n",
    "#     pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "#     return pe.type(dtype)\n",
    "\n",
    "# # classes for simpleViT architecture\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, dim, hidden_dim):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.LayerNorm(dim),\n",
    "#             nn.Linear(dim, hidden_dim),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(hidden_dim, dim),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "#         super().__init__()\n",
    "#         inner_dim = dim_head *  heads\n",
    "#         self.heads = heads\n",
    "#         self.scale = dim_head ** -0.5\n",
    "#         self.norm = nn.LayerNorm(dim)\n",
    "#         self.attend = nn.Softmax(dim = -1)\n",
    "#         self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "#         self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.norm(x)\n",
    "#         qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "#         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "#         dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "#         attn = self.attend(dots)\n",
    "#         out = torch.matmul(attn, v)\n",
    "#         out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "#         return self.to_out(out)\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "#         super().__init__()\n",
    "#         self.norm = nn.LayerNorm(dim)\n",
    "#         self.layers = nn.ModuleList([])\n",
    "#         for _ in range(depth):\n",
    "#             self.layers.append(nn.ModuleList([\n",
    "#                 Attention(dim, heads = heads, dim_head = dim_head),\n",
    "#                 FeedForward(dim, mlp_dim)\n",
    "#             ]))\n",
    "#     def forward(self, x):\n",
    "#         for attn, ff in self.layers:\n",
    "#             x = attn(x) + x\n",
    "#             x = ff(x) + x\n",
    "#         return self.norm(x)\n",
    "\n",
    "# class SimpleViT(nn.Module):\n",
    "#     def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
    "#         super().__init__()\n",
    "#         image_height, image_width = pair(image_size)\n",
    "#         patch_height, patch_width = pair(patch_size)\n",
    "#         assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "#         patch_dim = channels * patch_height * patch_width\n",
    "#         self.to_patch_embedding = nn.Sequential(\n",
    "#             Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = patch_height, p2 = patch_width),\n",
    "#             nn.LayerNorm(patch_dim),\n",
    "#             nn.Linear(patch_dim, dim),\n",
    "#             nn.LayerNorm(dim),\n",
    "#         )\n",
    "#         self.pos_embedding = posemb_sincos_2d(\n",
    "#             h = image_height // patch_height,\n",
    "#             w = image_width // patch_width,\n",
    "#             dim = dim,\n",
    "#         ) \n",
    "#         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "#         self.pool = \"mean\"\n",
    "#         self.to_latent = nn.Identity()\n",
    "#         self.linear_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         device = img.device\n",
    "#         x = self.to_patch_embedding(img)\n",
    "#         x += self.pos_embedding.to(device, dtype=x.dtype)\n",
    "#         x = self.transformer(x)\n",
    "#         x = x.mean(dim = 1)\n",
    "#         x = self.to_latent(x)\n",
    "#         return self.linear_head(x)\n",
    "\n",
    "    \n",
    "# class SimpleViTaps(nn.Module):\n",
    "#     def __init__(self, *, image_size, patch_size, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
    "#         super().__init__()\n",
    "#         self.image_height, self.image_width = pair(image_size)\n",
    "#         self.patch_height, self.patch_width = pair(patch_size)\n",
    "#         self.channels = channels\n",
    "#         self.dim = dim\n",
    "#         self.depth = depth\n",
    "#         self.heads = heads\n",
    "#         self.mlp_dim = mlp_dim\n",
    "#         self.dim_head = dim_head\n",
    "#         assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "#         patch_dim = self.channels * self.patch_height * self.patch_width\n",
    "#         self.to_patch_embedding = nn.Sequential(\n",
    "#             Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = self.patch_height, p2 = self.patch_width),\n",
    "#             nn.LayerNorm(patch_dim),\n",
    "#             nn.Linear(patch_dim, self.dim),\n",
    "#             nn.LayerNorm(self.dim),\n",
    "#         )\n",
    "#         self.pos_embedding = posemb_sincos_2d(\n",
    "#             h = self.image_height // self.patch_height,\n",
    "#             w = self.image_width // self.patch_width,\n",
    "#             dim = self.dim,\n",
    "#         ) \n",
    "#         self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim)\n",
    "#         self.pool = \"mean\"\n",
    "#         self.to_latent = nn.Identity()\n",
    "#         # Change this line to output a tensor with the same shape as the input\n",
    "#         self.linear_head = nn.Linear(self.dim, self.channels * self.image_height * self.image_width)\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         device = img.device\n",
    "#         x = self.to_patch_embedding(img)\n",
    "#         x += self.pos_embedding.to(device, dtype=x.dtype)\n",
    "#         x = self.transformer(x)\n",
    "#         x = x.mean(dim = 1)\n",
    "#         x = self.to_latent(x)\n",
    "#         x = self.linear_head(x)\n",
    "#         # Reshape the output to match the input dimensions\n",
    "#         x = x.view(-1, self.channels, self.image_height, self.image_width)\n",
    "#         return x    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# class CustomDatasetAPS(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None, multiplicity=1):\n",
    "#         \"\"\"\n",
    "#         Arguments:\n",
    "#             root_dir (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied on a sample. In my code transformAPS is used, doesn't do much but left up to continued work of project to experiment there, check final notes section\n",
    "#         \"\"\"\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.subs1 = {'train': 'train/', 'test': 'test/'}\n",
    "#         self.subs2 = {'residues': 'class_0/', 'SignalNoise': 'class_1/', 'Noise': 'class_2/'}\n",
    "#         self.FullPaths = {'TrainR': root_dir+self.subs1['train']+self.subs2['residues'],\n",
    "#                          'TestR': root_dir+self.subs1['test']+self.subs2['residues'],\n",
    "#                          'TrainSN': root_dir+self.subs1['train']+self.subs2['SignalNoise'],\n",
    "#                          'TestSN': root_dir+self.subs1['test']+self.subs2['SignalNoise'],\n",
    "#                          'TrainN': root_dir+self.subs1['train']+self.subs2['Noise'],\n",
    "#                          'TestN': root_dir+self.subs1['test']+self.subs2['Noise'],}\n",
    "#         self.TrainrFiles = os.listdir(self.FullPaths['TrainR'])\n",
    "#         self.TestrFiles = os.listdir(self.FullPaths['TestR'])\n",
    "#         self.TrainsnFiles = os.listdir(self.FullPaths['TrainSN'])\n",
    "#         self.TestsnFiles = os.listdir(self.FullPaths['TestSN'])\n",
    "#         self.TrainnFiles = os.listdir(self.FullPaths['TrainN'])\n",
    "#         self.TestnFiles = os.listdir(self.FullPaths['TestN'])\n",
    "#         self.TrainrFiles = [file for file in self.TrainrFiles if os.path.isfile(os.path.join(self.FullPaths['TrainR'], file))]\n",
    "#         self.TestrFiles = [file for file in self.TestrFiles if os.path.isfile(os.path.join(self.FullPaths['TestR'], file))]\n",
    "#         self.TrainsnFiles = [file for file in self.TrainsnFiles if os.path.isfile(os.path.join(self.FullPaths['TrainSN'], file))]\n",
    "#         self.TestsnFiles = [file for file in self.TestsnFiles if os.path.isfile(os.path.join(self.FullPaths['TestSN'], file))]\n",
    "#         self.TrainnFiles = [file for file in self.TrainnFiles if os.path.isfile(os.path.join(self.FullPaths['TrainN'], file))]\n",
    "#         self.TestnFiles = [file for file in self.TestnFiles if os.path.isfile(os.path.join(self.FullPaths['TestN'], file))]\n",
    "#         self.FullList = self.TrainrFiles + self.TestrFiles + self.TrainsnFiles + self.TestsnFiles + self.TrainnFiles + self.TestnFiles\n",
    "#         self.multiplicity = multiplicity\n",
    "                \n",
    "#     def __len__(self):\n",
    "#         return len(self.TrainsnFiles)\n",
    "     \n",
    "#     def _multiNoise_(self, SNimg):\n",
    "#         std = torch.std(SNimg)\n",
    "#         noisy_channels = []\n",
    "#         for _ in range(self.multiplicity):\n",
    "#             std = torch.std(SNimg)\n",
    "#             noise_samples = torch.randn(self.multiplicity, *SNimg.shape)\n",
    "#             noisy_channels = SNimg + noise_samples * std\n",
    "#             noisy_channels = noisy_channels.squeeze(1)\n",
    "#             return noisy_channels\n",
    "\n",
    "#     def _SignalStack_(self, Rimg):\n",
    "#         return Rimg.unsqueeze(1).expand(-1, self.multiplicity, -1, -1).squeeze(0)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         trainSNpath = self.FullPaths['TrainSN'] + self.TrainsnFiles[idx]\n",
    "#         trainRpath = self.FullPaths['TrainR'] + self.TrainrFiles[idx]\n",
    "#         SNimg = Image.open(trainSNpath)\n",
    "#         Rimg = Image.open(trainRpath)\n",
    "#         if self.transform:\n",
    "#             SNimg = self.transform(SNimg)\n",
    "#             Rimg = self.transform(Rimg)\n",
    "#         noisy_SNimg = self._multiNoise_(SNimg)\n",
    "#         SignalStack = self._SignalStack_(Rimg)\n",
    "#         return noisy_SNimg, SignalStack   \n",
    "        \n",
    "        \n",
    "#      #what do i want to plot? (functions to call)\n",
    "# #plot example of original SNimg, original Rimg (both only 1 channel), Nimg = SNimg-Rimg next to each other. by using either the CustomDatasetAPS.__getitem__ or other standardized methods\n",
    "# #plot set of channels from noisy_SNimg, SignalStack from CustomDatasetAPS.__getitem__, plot them all in a large mosaic where user can specify how many images from noisy_SNimg, SignalStack tensors\n",
    "# #plot channel-wise average of noisy_SNimg, SignalStack from CustomDatasetAPS.__getitem__\n",
    "# #plot set of model(noisy_SNimg), SignalStack; in other words pass noisy_SNimg tensor through model\n",
    "# #plot (all batches) loss vs epoch   \n",
    "        \n",
    "# ###########################################################################################################################################################################        \n",
    "\n",
    "\n",
    "\n",
    "# #set of all plotting functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #note that dataset item retrieval function __getitem__ retrieves SN image, R image, does the multiplicity based noise resample so thing being saved here is \n",
    "# #(copy since long lasting)stack of Residuals and the resampled noise SNimg stack where channels=Multiplicity\n",
    "# #so save the thing across all new channels as a gif where each channel is a different image in gif sequence\n",
    "# #saves to root directory currently\n",
    "# def SaveGIF_inROOT_SN_R_N(datasetITEMidx): \n",
    "#     transformAPS = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),])\n",
    "#     #set up dataset object and dataloader\n",
    "#     Classified_APS_Path = DEFAULT_DATA_ROOTaddress + 'Png_n_sn_r/'\n",
    "#     Classified_APS_Dataset = CustomDatasetAPS(root_dir=Classified_APS_Path, transform=transformAPS, multiplicity = Multiplicity)\n",
    "#     SNstack, Rstack = Classified_APS_Dataset.__getitem__(datasetITEMidx)\n",
    "#     Nstack = SNstack - Rstack\n",
    "#     SNstack = (SNstack - SNstack.min()) / (SNstack.max() - SNstack.min())\n",
    "#     Rstack = (Rstack - Rstack.min()) / (Rstack.max() - Rstack.min())\n",
    "#     Nstack = (Nstack - Nstack.min()) / (Nstack.max() - Nstack.min())\n",
    "#     frames = []\n",
    "#     for i in range(SNstack.shape[0]):\n",
    "#         fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "#         fig.suptitle('dataset item ' + str(datasetITEMidx) + ' channel 0 to ' + str(Multiplicity-1) + '[Multiplicity-1]')\n",
    "#         axs[0].imshow(SNstack[i], cmap='gray')\n",
    "#         axs[0].set_title('SNimg' + ' 0 to ' + str(SNstack.shape[0]-1))\n",
    "#         axs[1].imshow(Rstack[i], cmap='gray')\n",
    "#         axs[1].set_title('Rimg'+ ' 0 to ' + str(SNstack.shape[0]-1))\n",
    "#         axs[2].imshow(Nstack[i], cmap='gray')\n",
    "#         axs[2].set_title('Nimg'+ ' 0 to ' + str(SNstack.shape[0]-1))\n",
    "#         fig.canvas.draw()\n",
    "#         image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8').reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "#         frames.append(image)\n",
    "#         plt.close(fig)\n",
    "#     imageio.mimsave(DEFAULT_DATA_ROOTaddress+'imgTensor'+str(datasetITEMidx)+'.gif', frames, 'GIF', duration=0.1)\n",
    "#     return None\n",
    "\n",
    "# #function to plot average of resampled Signal+noise images, Noise images, and Residuals\n",
    "# def plot_channelAVGs(datasetITEMidx):\n",
    "#     SNstack, Rstack = Classified_APS_Dataset.__getitem__(datasetITEMidx)\n",
    "#     Nstack = SNstack - Rstack\n",
    "#     SNstack = (SNstack - SNstack.min()) / (SNstack.max() - SNstack.min())\n",
    "#     Rstack = (Rstack - Rstack.min()) / (Rstack.max() - Rstack.min())\n",
    "#     Nstack = (Nstack - Nstack.min()) / (Nstack.max() - Nstack.min())\n",
    "#     avg_SNstack = SNstack.mean(dim=0)\n",
    "#     avg_SignalStack = Rstack.mean(dim=0)\n",
    "#     avg_Nstack = Nstack.mean(dim=0)\n",
    "#     fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "#     axs[0].imshow(avg_SNstack, cmap='gray')\n",
    "#     axs[0].set_title('Average SNimg')\n",
    "#     axs[1].imshow(avg_SignalStack, cmap='gray')\n",
    "#     axs[1].set_title('Average SignalStack')\n",
    "#     axs[2].imshow(avg_Nstack, cmap='gray')\n",
    "#     axs[2].set_title('Average Nimg')\n",
    "#     plt.show()\n",
    "    \n",
    "# #function to plot the signal averages across all the N=Multiplicity image channels, and compare with similar average across the model(Signal+Noise) result tensor\n",
    "# def plot_channelAVGsVSmodel(datasetITEMidx, epochNum, dataSet, Model):\n",
    "#     SNstack, Rstack = dataSet.__getitem__(datasetITEMidx)\n",
    "#     Nstack = SNstack - Rstack\n",
    "#     # Add an extra dimension for the batch size\n",
    "#     SNstack = SNstack.unsqueeze(0)\n",
    "#     ModelStack = Model(SNstack)\n",
    "#     # Remove the batch dimension for plotting\n",
    "#     ModelStack = ModelStack.squeeze(0)\n",
    "#     SNstack = SNstack.squeeze(0)\n",
    "#     SNstack = (SNstack - SNstack.min()) / (SNstack.max() - SNstack.min())\n",
    "#     Rstack = (Rstack - Rstack.min()) / (Rstack.max() - Rstack.min())\n",
    "#     Nstack = (Nstack - Nstack.min()) / (Nstack.max() - Nstack.min())\n",
    "#     ModelStack = (ModelStack - ModelStack.min()) / (ModelStack.max() - ModelStack.min())\n",
    "#     #print(SNstack.shape, Rstack.shape, Nstack.shape, ModelStack.shape)\n",
    "#     avg_SNstack = SNstack.mean(dim=0)\n",
    "#     avg_SignalStack = Rstack.mean(dim=0)\n",
    "#     avg_Nstack = Nstack.mean(dim=0)\n",
    "#     avg_ModelStack = ModelStack.mean(dim=0)\n",
    "#     #print(avg_SNstack.shape, avg_SignalStack.shape, avg_Nstack.shape, avg_ModelStack.shape)\n",
    "#     fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "#     fig.suptitle('dataset item ' + str(datasetITEMidx) + ' channel averages', fontsize=16, fontweight='bold', y=0.75, verticalalignment='center', horizontalalignment='center')\n",
    "#     axs[0].imshow(avg_SNstack, cmap='gray')\n",
    "#     axs[0].set_title('Average SNimg')\n",
    "#     axs[1].imshow(avg_SignalStack, cmap='gray')\n",
    "#     axs[1].set_title('Average SignalStack')\n",
    "#     axs[2].imshow(avg_Nstack, cmap='gray')\n",
    "#     axs[2].set_title('Average Nimg')\n",
    "#     #axs[3].imshow(avg_ModelStack, cmap='gray')\n",
    "#     axs[3].imshow(avg_ModelStack.detach().numpy(), cmap='gray')\n",
    "#     axs[3].set_title('Model Output AVG')\n",
    "#     plt.savefig(DEFAULT_DATA_ROOTaddress+'AVGcomparison'+str(datasetITEMidx)+'epoch'+str(epochNum)+'.png')\n",
    "#     plt.show()\n",
    "#     plt.clf()\n",
    "    \n",
    "    \n",
    "\n",
    "# #the following function is meant to encapsulate the entire process from datm/gatm directory management, data prep, training, and plotting\n",
    "# def trainModelFromScratch(rootDir, imgDim, train_test_ratio, setting, image_size, patch_size, dim, depth, heads, mlp_dim, Multiplicity, channels, dim_head,num_epochs, LearnRate, BatchNum, intermediateIDX=0):  \n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     loaderFULL(rootDir, imgDim, train_test_ratio, setting)\n",
    "#     # Create an instance of the SimpleViT model\n",
    "#     model = SimpleViTaps(image_size=image_size,patch_size=patch_size,dim=dim,depth=depth,heads=heads,mlp_dim=mlp_dim,channels=channels,dim_head=dim_head)\n",
    "#     #transformation initially applied to each sample image as it is fetched by \n",
    "#     transformAPS = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),])\n",
    "#     #set up dataset object and dataloader\n",
    "#     Classified_APS_Path = DEFAULT_DATA_ROOTaddress + 'Png_n_sn_r/'\n",
    "#     Classified_APS_Dataset = CustomDatasetAPS(root_dir=Classified_APS_Path, transform=transformAPS, multiplicity = Multiplicity)\n",
    "#     train_loader = DataLoader(Classified_APS_Dataset, batch_size=BatchNum, shuffle=True,drop_last=True) \n",
    "#     # Move the model to GPU if available\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     # Define the loss function and optimizer\n",
    "#     criterion = nn.MSELoss() \n",
    "#     optimizer = optim.Adam(model.parameters(), lr=LearnRate)\n",
    "#     lossAVG = []\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         lossEpoch = []\n",
    "#         for SN, R in train_loader:\n",
    "#             SN, R = SN.to(device), R.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(SN)\n",
    "#             loss = torch.sqrt(criterion(outputs, R))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             # Append the loss to the list of losses\n",
    "#             lossEpoch.append(loss.item())\n",
    "#         lossAVG.append(np.mean(lossEpoch))\n",
    "#         plot_channelAVGsVSmodel(intermediateIDX, epochNum = epoch, dataSet = Classified_APS_Dataset, Model = model)\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "#     modelFilename = 'APSmodel_' + timestamp\n",
    "#     torch.save(model, rootDir + modelFilename)\n",
    "#     # After training, plot the losses\n",
    "#     plt.plot(lossAVG)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('batch-avg Loss vs. Epoch during training')\n",
    "#     # Generate a unique filename with the current date and time\n",
    "#     filename = f'LossAVGvsEpoch_{timestamp}.png'\n",
    "#     # Save the plot with the updated filename\n",
    "#     plt.savefig(DEFAULT_DATA_ROOTaddress+filename)\n",
    "#     plt.show()\n",
    "#     plt.clf()\n",
    "        \n",
    "        \n",
    "# #useful if model already trained & want to conduct further training or use as initial condition for more testing or other purposes        \n",
    "# def loadTrainedModel(rootDir, modelFilename):\n",
    "#     # Later, when loading the model:\n",
    "#     loaded_model = torch.load(rootDir + modelFilename)\n",
    "    \n",
    "    \n",
    "# def load_data(DATM_GATM_address, imgDimensions):\n",
    "#     folderNoise = DATM_GATM_address + 'gatm/'\n",
    "#     folderSignalandNoise = DATM_GATM_address + 'datm/'\n",
    "#     filesSN = [file for file in os.listdir(folderSignalandNoise) if os.path.isfile(os.path.join(folderSignalandNoise, file))]\n",
    "#     filesN = [file for file in os.listdir(folderNoise) if os.path.isfile(os.path.join(folderNoise, file))]\n",
    "#     img_rows, img_cols, channels = imgDimensions\n",
    "#     allNoise = np.zeros((img_rows, img_cols,len(filesN)))\n",
    "#     allSignalNoise = np.zeros((img_rows, img_cols,len(filesSN)))\n",
    "\n",
    "#     count=0\n",
    "#     for file in filesN:\n",
    "#         pathNow = folderNoise + file\n",
    "#         filNow = xr.open_dataset(pathNow, engine='netcdf4')\n",
    "#         Znow = filNow['z']\n",
    "#         allNoise[:,:,count] = Znow\n",
    "#         count += 1    \n",
    "\n",
    "#     count=0\n",
    "#     for file in filesSN:\n",
    "#         pathNow = folderSignalandNoise + file\n",
    "#         filNow = xr.open_dataset(pathNow, engine='netcdf4')\n",
    "#         Znow = filNow['z']\n",
    "#         allSignalNoise[:,:,count] = Znow\n",
    "#         count += 1 \n",
    "\n",
    "#     allResidues = allSignalNoise - allNoise\n",
    "#     return allNoise, allSignalNoise, allResidues\n",
    "\n",
    "# def plot_images(address, allNoise, allSignalNoise, allResidues,idx=0):\n",
    "#     fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "#     fig.suptitle('Sample: ' + str(idx))\n",
    "#     im1 = axs[0].imshow(allNoise[:,:,idx], cmap='gray')\n",
    "#     axs[0].set_title('Noise')\n",
    "#     fig.colorbar(im1, ax=axs[0])\n",
    "#     im2 = axs[1].imshow(allSignalNoise[:,:,idx], cmap='gray')\n",
    "#     axs[1].set_title('Signal + Noise')\n",
    "#     fig.colorbar(im2, ax=axs[1])\n",
    "#     im3 = axs[2].imshow(allResidues[:,:,idx], cmap='gray')\n",
    "#     axs[2].set_title('Residues')\n",
    "#     fig.colorbar(im3, ax=axs[2])\n",
    "#     plt.savefig(address + 'sample' + str(idx) + '.png')\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.close()\n",
    "    \n",
    "\n",
    "# # usage\n",
    "# #DATM_GATM_address =DATM_GATM_address\n",
    "# #imgDimensions = [40,40,1]\n",
    "# #USERidx = 3\n",
    "# #allNoise, allSignalNoise, allResidues = load_data(DATM_GATM_address, imgDimensions)\n",
    "# #plot_images(DATM_GATM_address, allNoise, allSignalNoise, allResidues,idx = USERidx)\n",
    "\n",
    "\n",
    "# def plotMany(address, allNoise, allSignalNoise, allResidues, init, fin):\n",
    "#     for i in range (init,fin):\n",
    "#         plot_images(address, allNoise, allSignalNoise, allResidues,idx = i)\n",
    "        \n",
    "# #plotMany(DATM_GATM_address, allNoise, allSignalNoise, allResidues,0,10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680451d8-3de9-4cc3-a41b-ca65d7c4d4ed",
   "metadata": {},
   "source": [
    "[section 4]\n",
    "\n",
    "The following block of code is where the user may run the model from scratch on a root/datm,gatm folder with hyperparams of their choice  \n",
    "and continue with code via 'what next' block at bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365c8773-666b-45b3-99fd-f3e910e0334e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainModelFromScratch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m LearnRate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;66;03m#how quickly or slowly does model training try to move around, step size around learning space\u001b[39;00m\n\u001b[1;32m     37\u001b[0m intermediateIDX \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;66;03m#which image from the training set is used for the example plots, the channel averages of SNimg, SignalStack, Nimg, and model(SNimg)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrainModelFromScratch\u001b[49m(DEFAULT_DATA_ROOTaddress, imgDim, train_test_ratio, setting, image_size, patch_size, dim, depth, heads, mlp_dim, Multiplicity, channels, dim_head,num_epochs, LearnRate, BatchNum, intermediateIDX)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainModelFromScratch' is not defined"
     ]
    }
   ],
   "source": [
    "#here is a code block which, after running cells above to define the class and functions, may be used to run the model from scratch\n",
    "#simply point to the DATM_GATM root directory and provide parameters\n",
    "##WHAT SHOULD HAPPEN\n",
    "##image files in form of .grd in gatm/datm folders copied to PNG format & saved in pytorch compatible directory structure\n",
    "##model is prepped and trained\n",
    "##plots produced and saved which correspond to channel averages of the Signal+noise, residue, noise, and model(signal+noise) for each epoch/iteration of training\n",
    "##final plot is produces to show convergence of loss vs epochs\n",
    "\n",
    "#if one wishes to see more detail regarding how images look, how resampling of noise work, etc\n",
    "#please use plotting functions such as 'SaveGIF_inROOT_SN_R_N' which produces a gif of all the channels of the stack of resampled noise Signal+Noise images\n",
    "#or try to use some of the architecture of the 'trainModelFromScratch' function to step through things in more detail\n",
    "\n",
    "#####################please specify folder where gatm/datm data folders are located\n",
    "#DEFAULT_DATA_ROOTaddress =  'D:\\\\Research\\\\Yuri Fialko\\\\finalTesting\\\\'\n",
    "#C:\\Users\\lukef\\Google Drive\\UCSD\\research\\SIO\\Yuri Fialko\\APS_removal_via_Pytorch_ViT\n",
    "# DEFAULT_DATA_ROOTaddress =  'C:\\\\Users\\\\lukef\\\\Google Drive\\\\UCSD\\\\research\\\\SIO\\\\Yuri Fialko\\\\APS_removal_via_Pytorch_ViT\\\\'\n",
    "DEFAULT_DATA_ROOTaddress =  '/Users/evavra/Software/APS_removal_via_Pytorch_ViT/'\n",
    "\n",
    "#image parameters\n",
    "imgDim = [40,40,1]\n",
    "train_test_ratio = 0.8\n",
    "setting = 'DATM_GATM'\n",
    "\n",
    "# Define the model parameters\n",
    "image_size = (imgDim[0], imgDim[1])  #match to host dataset\n",
    "patch_size = (10, 10)  #pixel size of patches used for feature embedding\n",
    "dim = 256  # dimensionality of the token embeddings which are extracted from the patches\n",
    "depth = 6 #number of transformer layers in the model, deeper models encode the image data further so capture more complex features\n",
    "heads = 8 #number of attention heads in the multi-head self-attention mechanism. Each head attends to different parts of the input sequence, allowing the model to capture diverse patterns and relationships\n",
    "mlp_dim = 512  #dimension of the feed-forward neural network (multi-layer perceptron) within the transformer block. This network processes the output of the self-attention mechanism. increase for more complexity\n",
    "Multiplicity = 24    #this resamples spatial noise in image to simulate effect of CSS image stacking, treat Multiplicity as number of images in time sequence with new random noise each time\n",
    "channels = Multiplicity  #this is redundancy, but wanted to keep the naming conventions of the pytorch functions intact to a certain degree, images with multiplicity = N have their tensors extended from (batchNum, channel=1, height, width) to (batchNum, channel=Multiplicity, height, width) such that the ML network is experiencing time sequences of data\n",
    "dim_head = 64 #dimension of each attention head. It determines how the attention mechanism splits the input embeddings. A common choice is 64, but it can vary based on the specific architecture and problem\n",
    "num_epochs = 20  #number of training epochs (training loops)\n",
    "BatchNum = 50   #how many images are used in each batch of training\n",
    "LearnRate = 0.001 #how quickly or slowly does model training try to move around, step size around learning space\n",
    "intermediateIDX = 30 #which image from the training set is used for the example plots, the channel averages of SNimg, SignalStack, Nimg, and model(SNimg)\n",
    "\n",
    "trainModelFromScratch(DEFAULT_DATA_ROOTaddress, imgDim, train_test_ratio, setting, image_size, patch_size, dim, depth, heads, mlp_dim, Multiplicity, channels, dim_head,num_epochs, LearnRate, BatchNum, intermediateIDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80438e4f-c9c9-4c8c-84a6-4ea67f5a9f55",
   "metadata": {},
   "source": [
    "below is code cells where the user may continue coding or testing things      \n",
    "in other words more freeform cells, a few examples provided but otherwise one may proceed to \"what's next\" section    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13c135-9e25-4bae-b474-a7731b1abff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to make gifs of all the training images where gif scans across noise-resampled channels of image\n",
    "\n",
    "trainDataLengthLuke = 400\n",
    "\n",
    "for i in range(0,trainDataLengthLuke):\n",
    "    SaveGIF_inROOT_SN_R_N(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a695c-24cc-4830-86b3-505e86d2def0",
   "metadata": {},
   "source": [
    "#if one is done training and wants to load in a previously trained model try      \n",
    "loadTrainedModel(rootDir, modelFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c086c9-2f2b-4964-85e7-38a0195fa842",
   "metadata": {},
   "source": [
    "testing code for a model not inside of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40062573-f404-4a5b-96ca-1f3326e3c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#loaderFULL(DEFAULT_DATA_ROOTaddress, imgDim, train_test_ratio, setting)\n",
    "# Create an instance of the SimpleViT model\n",
    "model = SimpleViTaps(image_size=image_size,patch_size=patch_size,dim=dim,depth=depth,heads=heads,mlp_dim=mlp_dim,channels=channels,dim_head=dim_head)\n",
    "#transformation initially applied to each sample image as it is fetched by \n",
    "transformAPS = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),])\n",
    "#set up dataset object and dataloader\n",
    "Classified_APS_Path = DEFAULT_DATA_ROOTaddress + 'Png_n_sn_r/'\n",
    "Classified_APS_Dataset = CustomDatasetAPS(root_dir=Classified_APS_Path, transform=transformAPS, multiplicity = Multiplicity)\n",
    "train_loader = DataLoader(Classified_APS_Dataset, batch_size=BatchNum, shuffle=True,drop_last=True) \n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=LearnRate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#testing index used to check plotting functions, possible values [0,len(training_data)-1]\n",
    "testIDX = 2\n",
    "\n",
    "SaveGIF_inROOT_SN_R_N(testIDX)\n",
    "\n",
    "plot_channelAVGs(testIDX)\n",
    "\n",
    "#plot_channelAVGsVSmodel(testIDX, epochNum=0)  \n",
    "\n",
    "\n",
    "plot_channelAVGsVSmodel(testIDX, epochNum = 0, dataSet = Classified_APS_Dataset, Model = model)\n",
    "\n",
    "lossAVG = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    lossEpoch = []\n",
    "    for SN, R in train_loader:\n",
    "        SN, R = SN.to(device), R.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(SN)\n",
    "        loss = torch.sqrt(criterion(outputs, R))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            # Append the loss to the list of losses\n",
    "        lossEpoch.append(loss.item())\n",
    "    lossAVG.append(np.mean(lossEpoch))\n",
    "    plot_channelAVGsVSmodel(testIDX, epochNum = epoch, dataSet = Classified_APS_Dataset, Model = model)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    #modelFilename = 'APSmodel_' + timestamp\n",
    "    #torch.save(model, rootDir + modelFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a3d8e-c0d1-43b4-806d-3a2f36f355ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need code for organizing data, if data already prepped into root/train,test/class_0, class_1, class_2 directory structure you may move on to run ML code blocks past the next markdown cell\n",
    "\n",
    "####this address is associated with the data\n",
    "#\n",
    "#EFAULT_DATA_ROOTaddress = 'D:\\\\Research\\\\Yuri Fialko\\\\finalTesting\\\\'\n",
    "\n",
    "#image dimensions [x,y,channel], testing was [40,40,1]\n",
    "#mgDim = [40,40,1]\n",
    "#rain_test_ratio = 0.8\n",
    "#etting = 'DATM_GATM' #change to 'RAW_CSS' in case of images which are Signal # noise and corresponding images of CSS stack average, \n",
    "#both should have dimensionality (height,width,channel=1), matching number of samples vs corresponding CSS images\n",
    "#IN EITHER CASE, data should be in root/subdirectory corresponding to these naming conventions, namely 'datm/gatm/raw/css'\n",
    "##datm - signal+noise\n",
    "##gatm - noise\n",
    "##raw - relatively unaltered images such as real examples from satellite\n",
    "##css - way of replicating 'residue' images by utilizing CSS output that corresponds to the 'raw' output's interferometric fault line displacement signal\n",
    "\n",
    "###load in and preprocess data from example datm/gatm provided\n",
    "#oaderFULL(DEFAULT_DATA_ROOTaddress, imgDim, train_test_ratio, setting)\n",
    "#what should happen is that images are organized into root/train,test/class_0, class_1, class_2 where class 0 is residues, class 1 is signal+noise, class 2 is noise\n",
    "#this is the directory structure expected by pytorch dataloader\n",
    "#can simply modify code to create new subdirectory 'valid' and populate it with some of the data by modifying code in previous function block^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f110353-fb15-4fcb-8e4d-6eb227160b1c",
   "metadata": {},
   "source": [
    "Recommendations for next steps:     \n",
    "\n",
    "-hyperparameter optimization such as by grid search over order of magnitude ranges, or more sophisticated methods such as differential evolution, MCMC, outer layer of loss optimization training, etc...       \n",
    "---Note: below are some more easily identifiable from pytorch architecture, but some other hyperparams exist within code which are important such as 'multiplicity'           \n",
    "---learning rate => how quickly or slowly does model training try to move around, step size around learning space. not finding global optimum need to increase, if can't quite fall down into nice stable spot need to slow down/decrease       \n",
    "---batch size =>   how many images are used in each batch during training     \n",
    "---epochs  => how many training loops run, increase if want to converge more     \n",
    "---loss function or loss ensemble(torch MSE used)         \n",
    "---optimizer(Adam used)       \n",
    "---patch_size => how large are the moving windows of the transform which encodes the pixel data into features? NEEDS TO FIT NEATLY IN IMAGE ie no remainder between img/patch  \n",
    "---dim => dimensionality of the token embeddings which are extracted from the patches     \n",
    "---depth => number of transformer layers in the model, deeper models encode the image data further so capture more complex features     \n",
    "---heads => number of attention heads in the multi-head self-attention mechanism. Each head attends to different parts of the input sequence, allowing the model to capture diverse patterns and relationships     \n",
    "---mlp_dim => dimension of the feed-forward neural network (multi-layer perceptron) within the transformer block. This network processes the output of the self-attention mechanism. increase for more complexity      \n",
    "---channels => THIS SHOULD BE 'channels = multiplicity', see multiplicity section below     \n",
    "---dim_head => dimension of each attention head. It determines how the attention mechanism splits the input embeddings. A common choice is 64, but it can vary based on the specific architecture and problem   \n",
    "\n",
    "-insert model validation   \n",
    "---model currently has a rudimentary training loop, but some methods such as validation can be added to make model more robust & generalizable\n",
    "\n",
    "-modify ViT model       \n",
    "---much of the code provided in the folder is not strictly necessary to run my model, it is example code from https://github.com/lucidrains/vit-pytorch project       \n",
    "---the main reason I want to include this is because the authors do an exhaustive meta analysis of different ViT architectures, their github links to many of the papers which established these different models       \n",
    "---there are tons of models to try using & potentialy some maybe deal with APS removal particularly well, at least better than the simpleVIT used in development     \n",
    "---the model I used in development and testing is a modified version of their 'SimpleViT.py'       \n",
    "---the changes I made are as follows:       \n",
    "---rather than classification, my model tries to find the signal in the noise       \n",
    "---it uses 'self.linear_head = nn.Linear(self.dim, self.channels * self.image_height * self.image_width)' to output the transformed image tensor back to a tensor of the same original shape, to then be compared against the true residual image in the loss function during training (loss is nn.MSELoss())     \n",
    "---NOTE that the linear head method must be defined & called upon in the forward pass       \n",
    "---some other tweaks are small like no need to use num_classes stuff     \n",
    "---if a new model is not modified from classification to transform, issues of dimensionality will pop up when loss(model(input), residue) is called  \n",
    "---the classifier models return outputs with dimensionality (batchNum, classVec) to be compared with a TrueClassVec during classification tasks whereas my filtering model returns outputs with same dimensionality as original images ie (batchNum, channels, height, width) via the linear head change       \n",
    "------If I were messing around with different model examples, which I would likely do in the short term rather than long term of the project, I would try to reshape all/many ViT examples from github examples to the task of filtration/transform as done with the simple model and then use a dict to organize them within the function call for training, meaning use model as another parameter in the training/testing/use function call  \n",
    "-----for meta-analysis purposes and use in more complex systems since certains models might convey different advantages   \n",
    "-----could also use them for some sort of classification task such as classifying whether or not a signal is magnitude 0,1,2,...    \n",
    "-----or other tasks, please refer to https://github.com/lucidrains/vit-pytorch project for more info     \n",
    "\n",
    "-modify initial data transform    \n",
    "---[still need to rename the initial transformation other than transformMine]\n",
    "---this transform is initially applied to the data upon loading with the [CustomDatasetMine], which is when algo looks for subdirectories 'train/test' with parallel sub-sub directories 'class_0/class_1/2/3....']  \n",
    "---all this transform really does now is make sure the image_size parameters enforced by user input are holding with images   \n",
    "---and transform the images into tensors  \n",
    "---however; MANY transforms exist which may be easily applied to the images by simply adding some code along the lines of https://pytorch.org/vision/stable/transforms.html  \n",
    "---these transforms may serve as incredible ways to pre-process the data, generalize the model, and optimize performance  \n",
    "---if it were me  \n",
    "---I would test implementing different transforms   \n",
    "---optimize loss around them via an outer training loop over hyperparams such as ones associated with these transforms   \n",
    "---eventual goal of having some ensemble model which can deal with data that has been transformed in different ways from simple rotations to warping or other phenomena which are related to image defects in the real use case datasets   \n",
    "\n",
    "\n",
    "-modify test dataset from MATLAB fake_igrams code       \n",
    "---scale up images, save results, use to see how well model deals with data scaling       \n",
    "---make very large batches for larger testing runs for continued training       \n",
    "---vary shape of signal to make sure model isn't overfitting to gaussian bump or something, this is realistically addressed by true-signal training though, see below        \n",
    "\n",
    "-use on real data, best model for task will be one trained as close to real conditions as possible       \n",
    "---[advice on how to import data differently, need to design it in such a way that gatm & datm are used as root]       \n",
    "---I'm going to try to make it where you may simply point to the root folder of data, specifically one with 'datm' 'gatm' folders ie outpout of fake_igrams.m code & autoprocess data & prep for training     \n",
    "---the main thing that needs to happen to code if not loading correctly: training data loader objects expect directory structure of root/train/class_#   \n",
    "---where you may have train/valid/test subdirectories from root, then within those have class_0/class_1/....      \n",
    "---these classes were used to organize data for model, in my code class_0 is residue images, class_1 is signal and noise, class_2 is noise     \n",
    "---my goal with code is to make it where you may simple point to root with datm & gatm, then code will organize that folder structure for training     \n",
    "---[speaking of validation code and samples, they]       \n",
    "---when training on real data, would maybe use trustworthy CSS to approximate real signal, this would serve as 'Residue' in model & training ie. thing which is being compared in loss function against model(Signal+Noise)     \n",
    "---then for real use would want to simply save model parameters and structure then call as model(Signal+Noise_real) => model's best guess at what persistent signal looks like     \n",
    "\n",
    "-modify multiplicity parameter       \n",
    "---(relevant code CustomDatasetMine.multiNoise & CustomDatasetMine.SignalStack)       \n",
    "---multiplicity is effectively a method of using CSS methodology        \n",
    "---when you make multiplicity = n (integer), the 1-channel image input is turned into a n-channel image where pixel noise has been resampled for each new channel       \n",
    "---the STD of the original Signal_and_Noise image is extracted via 'torch.std(SNimg)'       \n",
    "---then the noise is resampled/overlaid with new noise layer via 'torch.randn(self.multiplicity, *SNimg.shape)'       \n",
    "---such that the image from get_item has channels = multiplicity & each new channel has resampled noise     \n",
    "---I hope this procedure is fine to resample noise, I figured it wasn't too much of issue since can tune parameters of original MATLAB file noise or tune scaling of these new noise layers by tuning the parameters of the functions I showed above^       \n",
    "---the residue signal (SignalNoise - Noise) does not resample noise since it's variations are miniscule, in other words signal is relatively long lasting/ persistent feature in image. Residue is simply extended to the new channel number w/ 'return Rimg.unsqueeze(1).expand(-1, self.multiplicity, -1, -1).squeeze(0)'     \n",
    "---the whole point of this multiplicity?       \n",
    "---simulate having a long lasting signal with each (time) snapshot resampling short timescale noise       \n",
    "---the idea being that in the use case of real-time satellite images we may simply take 'multiplicity' parameter to tell us how many of the previous snapshots to use in a stacked image object, keep old images in moving window       \n",
    "---essentially take advantage of CSS methodology to show model the signal since it's time persistence is the main difference we want model focused on wrt noise vs signal        \n",
    "---should be helpful for direct comparison of standardized CSS vs this model, ie. multiplicity is stand-in for 'stackNum' in CSS       \n",
    "\n",
    "-modify noise level to test robustness to variable noise       \n",
    "---scaling w/ a multiplicative parameter, train model on variable noise to keep it robust, but focus on real signal operating range     \n",
    "---form: current implementation is 'torch.randn(self.multiplicity, *SNimg.shape)'  with 'torch.std(SNimg)'  as noise scaling, please refer to CustomDatasetMine.multiNoise for noise modification     \n",
    "\n",
    "-compartmentalize code further       \n",
    "---this may depend on use case, but generally wrapping code blocks into functions or classes & importable files to enable implementation in other systems       \n",
    "\n",
    "-if computational power is needed consider cuda core or other hardware changes to help with training on high resolution or high multiplicity        \n",
    "---my device was able to handle development & testing, but as real data images come the model will need more compute to work with  \n",
    "\n",
    "-Visualization   \n",
    "---[I might have time to do some visualization codes]     \n",
    "\n",
    "-Train on different task?  \n",
    "---currently training is loss(model(SignalAndNoise), Residue)  \n",
    "---but maybe the model could be trained on a different task or multiple tasks?   \n",
    "---such as loss(model(SignalAndNoise), Noise)    \n",
    "---or a multi loss system  \n",
    "\n",
    "-Generative adversarial network  \n",
    "---train one model to produce noise as mentioned above in 'different tasks'^    \n",
    "---then set up system where generative model produces new noise overlays or new signals    \n",
    "---then set up a GAN loop between generative network and discriminator network  \n",
    "---might help explore wider state space of possible noise and increase generalizability/robustness  \n",
    "\n",
    "\n",
    "-  \n",
    "-  \n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739efefd-9e76-43fb-8a20-7d8be4c70f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
